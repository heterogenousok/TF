{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark_sql.ipynb  spark-warehouse  电商推荐.ipynb  推荐概述.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 items\r\n",
      "-rw-r--r--   1 root supergroup      88444 2021-03-29 21:19 /USA.json\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-03-11 21:10 /hbase\r\n",
      "-rw-r--r--   1 root supergroup       4662 2021-03-26 20:22 /iris.csv\r\n",
      "drwxr-xr-x   - root supergroup          0 2020-12-14 11:59 /output\r\n",
      "drwxr-xr-x   - root supergroup          0 2020-12-14 12:08 /output1\r\n",
      "drwxr-xr-x   - root supergroup          0 2020-12-14 14:00 /output2\r\n",
      "-rw-r--r--   1 root supergroup   22924462 2021-03-30 20:50 /raw_nyc_phil.json\r\n",
      "drwx------   - root supergroup          0 2020-12-21 13:00 /tmp\r\n",
      "drwxr-xr-x   - root supergroup          0 2020-12-16 11:12 /user\r\n",
      "-rw-r--r--   1 root supergroup         42 2020-12-14 11:36 /words\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/miniconda2/envs/py365/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 配置spark driver和pyspark运行时，所使用的python解释器路径\n",
    "PYSPARK_PYTHON = \"/miniconda2/envs/py365/bin/python\"\n",
    "JAVA_HOME='/root/bigdata/jdk'\n",
    "# 当存在多个版本时，不指定很可能会导致出错\n",
    "os.environ[\"PYSPARK_PYTHON\"] = PYSPARK_PYTHON\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = PYSPARK_PYTHON\n",
    "os.environ['JAVA_HOME']=JAVA_HOME\n",
    "# spark配置信息\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "#注意先启动standalone模式\n",
    "# ./start-master.sh -h 192.168.19.137\n",
    "#  ./start-slave.sh spark://192.168.19.137:7077\n",
    "SPARK_APP_NAME = \"preprocessingBehaviorLog\"\n",
    "SPARK_URL = \"spark://192.168.19.137:7077\"\n",
    "\n",
    "conf = SparkConf()    # 创建spark config对象\n",
    "config = (\n",
    "\t(\"spark.app.name\", SPARK_APP_NAME),    # 设置启动的spark的app名称，没有提供，将随机产生一个名称\n",
    "\t(\"spark.executor.memory\", \"6g\"),    # 设置该app启动时占用的内存用量，默认1g\n",
    "\t(\"spark.master\", SPARK_URL),    # spark master的地址\n",
    "    (\"spark.executor.cores\", \"2\"),    # 设置spark executor使用的CPU核心数\n",
    "    # 以下三项配置，可以控制执行器数量\n",
    "#     (\"spark.dynamicAllocation.enabled\", True),\n",
    "#     (\"spark.dynamicAllocation.initialExecutors\", 1),    # 1个执行器\n",
    "#     (\"spark.shuffle.service.enabled\", True)\n",
    "# \t('spark.sql.pivotMaxValues', '99999'),  # 当需要pivot DF，且值很多时，需要修改，默认是10000\n",
    ")\n",
    "# 查看更详细配置及说明：https://spark.apache.org/docs/latest/configuration.html\n",
    "\n",
    "conf.setAll(config)\n",
    "\n",
    "# 利用config对象，创建spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#通过RDD来创建Datarame\n",
    "#创建Datarase需要有sparg session\n",
    "#创刨建RDD蒜要sparkcontext\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "sc = spark.sparkContext\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", 6)\n",
    "# ================直接创建==========================\n",
    "l = [('Ankit',25),('Jalfaizy',22),('saurabh',20),('Bala',26)]\n",
    "rdd = sc.parallelize(l)\n",
    "#为数据添加列名\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))\n",
    "#创建DataFrame\n",
    "schemaPeople = spark.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaPeople"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|age|    name|\n",
      "+---+--------+\n",
      "| 25|   Ankit|\n",
      "| 22|Jalfaizy|\n",
      "| 20| saurabh|\n",
      "| 26|    Bala|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schemaPeople.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#默认从hadoop的根加载数据\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, SepalLength: string, SepalWidth: string, PetalLength: string, PetalWidth: string, Species: string, cls: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'SepalLength',\n",
       " 'SepalWidth',\n",
       " 'PetalLength',\n",
       " 'PetalWidth',\n",
       " 'Species',\n",
       " 'cls']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-----------+----------+-------+---+--------+\n",
      "| id|SepalLength|SepalWidth|PetalLength|PetalWidth|Species|cls|newWidth|\n",
      "+---+-----------+----------+-----------+----------+-------+---+--------+\n",
      "|  1|        5.1|       3.5|        1.4|       0.2| setosa|  0|     7.0|\n",
      "|  2|        4.9|         3|        1.4|       0.2| setosa|  0|     6.0|\n",
      "|  3|        4.7|       3.2|        1.3|       0.2| setosa|  0|     6.4|\n",
      "|  4|        4.6|       3.1|        1.5|       0.2| setosa|  0|     6.2|\n",
      "|  5|          5|       3.6|        1.4|       0.2| setosa|  0|     7.2|\n",
      "|  6|        5.4|       3.9|        1.7|       0.4| setosa|  0|     7.8|\n",
      "|  7|        4.6|       3.4|        1.4|       0.3| setosa|  0|     6.8|\n",
      "|  8|          5|       3.4|        1.5|       0.2| setosa|  0|     6.8|\n",
      "|  9|        4.4|       2.9|        1.4|       0.2| setosa|  0|     5.8|\n",
      "| 10|        4.9|       3.1|        1.5|       0.1| setosa|  0|     6.2|\n",
      "| 11|        5.4|       3.7|        1.5|       0.2| setosa|  0|     7.4|\n",
      "| 12|        4.8|       3.4|        1.6|       0.2| setosa|  0|     6.8|\n",
      "| 13|        4.8|         3|        1.4|       0.1| setosa|  0|     6.0|\n",
      "| 14|        4.3|         3|        1.1|       0.1| setosa|  0|     6.0|\n",
      "| 15|        5.8|         4|        1.2|       0.2| setosa|  0|     8.0|\n",
      "| 16|        5.7|       4.4|        1.5|       0.4| setosa|  0|     8.8|\n",
      "| 17|        5.4|       3.9|        1.3|       0.4| setosa|  0|     7.8|\n",
      "| 18|        5.1|       3.5|        1.4|       0.3| setosa|  0|     7.0|\n",
      "| 19|        5.7|       3.8|        1.7|       0.3| setosa|  0|     7.6|\n",
      "| 20|        5.1|       3.8|        1.5|       0.3| setosa|  0|     7.6|\n",
      "+---+-----------+----------+-----------+----------+-------+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============增加一列(或者替换) withColumn===========\n",
    "#定义一个新的列，数据为其他某列数据的两倍\n",
    "#如果操作的是原有列，可以替换原有列的数据\n",
    "df.withColumn('newWidth',df.SepalWidth * 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-----------+----------+-------+\n",
      "| id|SepalLength|SepalWidth|PetalLength|PetalWidth|Species|\n",
      "+---+-----------+----------+-----------+----------+-------+\n",
      "|  1|        5.1|       3.5|        1.4|       0.2| setosa|\n",
      "|  2|        4.9|         3|        1.4|       0.2| setosa|\n",
      "|  3|        4.7|       3.2|        1.3|       0.2| setosa|\n",
      "|  4|        4.6|       3.1|        1.5|       0.2| setosa|\n",
      "|  5|          5|       3.6|        1.4|       0.2| setosa|\n",
      "|  6|        5.4|       3.9|        1.7|       0.4| setosa|\n",
      "|  7|        4.6|       3.4|        1.4|       0.3| setosa|\n",
      "|  8|          5|       3.4|        1.5|       0.2| setosa|\n",
      "|  9|        4.4|       2.9|        1.4|       0.2| setosa|\n",
      "| 10|        4.9|       3.1|        1.5|       0.1| setosa|\n",
      "| 11|        5.4|       3.7|        1.5|       0.2| setosa|\n",
      "| 12|        4.8|       3.4|        1.6|       0.2| setosa|\n",
      "| 13|        4.8|         3|        1.4|       0.1| setosa|\n",
      "| 14|        4.3|         3|        1.1|       0.1| setosa|\n",
      "| 15|        5.8|         4|        1.2|       0.2| setosa|\n",
      "| 16|        5.7|       4.4|        1.5|       0.4| setosa|\n",
      "| 17|        5.4|       3.9|        1.3|       0.4| setosa|\n",
      "| 18|        5.1|       3.5|        1.4|       0.3| setosa|\n",
      "| 19|        5.7|       3.8|        1.7|       0.3| setosa|\n",
      "| 20|        5.1|       3.8|        1.5|       0.3| setosa|\n",
      "+---+-----------+----------+-----------+----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========删除一列  drop=========================\n",
    "#删除一列\n",
    "df.drop('cls').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+------------------+\n",
      "|summary|                id|       SepalLength|         SepalWidth|       PetalLength|        PetalWidth|  Species|               cls|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+------------------+\n",
      "|  count|               150|               150|                150|               150|               150|      150|               150|\n",
      "|   mean|              75.5| 5.843333333333335|  3.057333333333334|3.7580000000000027| 1.199333333333334|     null|               1.0|\n",
      "| stddev|43.445367992456916|0.8280661279778637|0.43586628493669793|1.7652982332594662|0.7622376689603467|     null|0.8192319205190406|\n",
      "|    min|                 1|               4.3|                  2|                 1|               0.1|   setosa|                 0|\n",
      "|    max|                99|               7.9|                4.4|               6.9|               2.5|virginica|                 2|\n",
      "+-------+------------------+------------------+-------------------+------------------+------------------+---------+------------------+\n",
      "\n",
      "+-------+------------------+\n",
      "|summary|               cls|\n",
      "+-------+------------------+\n",
      "|  count|               150|\n",
      "|   mean|               1.0|\n",
      "| stddev|0.8192319205190406|\n",
      "|    min|                 0|\n",
      "|    max|                 2|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#================ 统计信息 describe================\n",
    "df.describe().show()\n",
    "#计算某一列的描述信息\n",
    "df.describe('cls').show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|SepalLength|SepalWidth|\n",
      "+-----------+----------+\n",
      "|        5.1|       3.5|\n",
      "|        4.9|         3|\n",
      "|        4.7|       3.2|\n",
      "|        4.6|       3.1|\n",
      "|          5|       3.6|\n",
      "|        5.4|       3.9|\n",
      "|        4.6|       3.4|\n",
      "|          5|       3.4|\n",
      "|        4.4|       2.9|\n",
      "|        4.9|       3.1|\n",
      "|        5.4|       3.7|\n",
      "|        4.8|       3.4|\n",
      "|        4.8|         3|\n",
      "|        4.3|         3|\n",
      "|        5.8|         4|\n",
      "|        5.7|       4.4|\n",
      "|        5.4|       3.9|\n",
      "|        5.1|       3.5|\n",
      "|        5.7|       3.8|\n",
      "|        5.1|       3.8|\n",
      "+-----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===============提取部分列 select==============\n",
    "df.select('SepalLength','SepalWidth').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==================基本统计功能 distinct count=====\n",
    "df.select('cls').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------------------+\n",
      "|cls|max(SepalLength)|   avg(SepalWidth)|\n",
      "+---+----------------+------------------+\n",
      "|  0|             5.8| 3.428000000000001|\n",
      "|  1|               7|2.7700000000000005|\n",
      "|  2|             7.9|2.9739999999999998|\n",
      "+---+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分组统计 groupby(colname).agg({'col':'fun','col2':'fun2'})\n",
    "df.groupby('cls').agg({'SepalWidth':'mean','SepalLength':'max'}).show()\n",
    "\n",
    "# avg(), count(), countDistinct(), first(), kurtosis(),\n",
    "# max(), mean(), min(), skewness(), stddev(), stddev_pop(),\n",
    "# stddev_samp(), sum(), sumDistinct(), var_pop(), var_samp() and variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|width_count|distinct_cls_count|\n",
      "+-----------+------------------+\n",
      "|        150|                 3|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 自定义的汇总方法\n",
    "import pyspark.sql.functions as fn\n",
    "#调用函数并起一个别名,fn内部有哪些函数可以在pycharm联想\n",
    "df.agg(fn.count('SepalWidth').alias('width_count'),fn.countDistinct('cls').alias('distinct_cls_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================数据集拆成两部分 randomSplit ===========\n",
    "#设置数据比例将数据划分为两部分\n",
    "trainDF, testDF = df.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================采样数据 sample===========\n",
    "#withReplacement：是否有放回的采样\n",
    "#fraction：采样比例\n",
    "#seed：随机种子\n",
    "sdf = df.sample(False,0.2,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看两个数据集在类别上的差异 subtract，确保训练数据集覆盖了所有分类\n",
    "diff_in_train_test = testDF.select('cls').subtract(trainDF.select('cls'))\n",
    "diff_in_train_test.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|cls_SepalLength|4.3|4.4|4.5|4.6|4.7|4.8|4.9|  5|5.1|5.2|5.3|5.4|5.5|5.6|5.7|5.8|5.9|  6|6.1|6.2|6.3|6.4|6.5|6.6|6.7|6.8|6.9|  7|7.1|7.2|7.3|7.4|7.6|7.7|7.9|\n",
      "+---------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|              2|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  1|  1|  3|  1|  2|  2|  2|  6|  5|  4|  0|  5|  2|  3|  0|  1|  3|  1|  1|  1|  4|  1|\n",
      "|              1|  0|  0|  0|  0|  0|  0|  1|  2|  1|  1|  0|  1|  5|  5|  5|  3|  2|  4|  4|  2|  3|  2|  1|  2|  3|  1|  1|  1|  0|  0|  0|  0|  0|  0|  0|\n",
      "|              0|  1|  3|  1|  4|  2|  5|  4|  8|  8|  3|  1|  5|  2|  0|  2|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+---------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ================交叉表 crosstab=============\n",
    "# 一行代表一个类别，这个类别中sepallength的分布式怎么样的\n",
    "df.crosstab('cls','SepalLength').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|cls|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "+---+\n",
      "\n",
      "['0', '1']\n",
      "+---+-----------+----------+-----------+----------+---------+---+-------+\n",
      "| id|SepalLength|SepalWidth|PetalLength|PetalWidth|  Species|cls|New_cls|\n",
      "+---+-----------+----------+-----------+----------+---------+---+-------+\n",
      "|101|        6.3|       3.3|          6|       2.5|virginica|  2|      2|\n",
      "|102|        5.8|       2.7|        5.1|       1.9|virginica|  2|      2|\n",
      "|103|        7.1|         3|        5.9|       2.1|virginica|  2|      2|\n",
      "|104|        6.3|       2.9|        5.6|       1.8|virginica|  2|      2|\n",
      "|105|        6.5|         3|        5.8|       2.2|virginica|  2|      2|\n",
      "|106|        7.6|         3|        6.6|       2.1|virginica|  2|      2|\n",
      "|107|        4.9|       2.5|        4.5|       1.7|virginica|  2|      2|\n",
      "|108|        7.3|       2.9|        6.3|       1.8|virginica|  2|      2|\n",
      "|109|        6.7|       2.5|        5.8|       1.8|virginica|  2|      2|\n",
      "|110|        7.2|       3.6|        6.1|       2.5|virginica|  2|      2|\n",
      "|111|        6.5|       3.2|        5.1|         2|virginica|  2|      2|\n",
      "|112|        6.4|       2.7|        5.3|       1.9|virginica|  2|      2|\n",
      "|113|        6.8|         3|        5.5|       2.1|virginica|  2|      2|\n",
      "|114|        5.7|       2.5|          5|         2|virginica|  2|      2|\n",
      "|115|        5.8|       2.8|        5.1|       2.4|virginica|  2|      2|\n",
      "|116|        6.4|       3.2|        5.3|       2.3|virginica|  2|      2|\n",
      "|117|        6.5|         3|        5.5|       1.8|virginica|  2|      2|\n",
      "|118|        7.7|       3.8|        6.7|       2.2|virginica|  2|      2|\n",
      "|119|        7.7|       2.6|        6.9|       2.3|virginica|  2|      2|\n",
      "|120|          6|       2.2|          5|       1.5|virginica|  2|      2|\n",
      "+---+-----------+----------+-----------+----------+---------+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#================== 综合案例 + udf================\n",
    "# 测试数据集中有些类别在训练集中是不存在的，找到这些数据集做后续处理\n",
    "trainDF,testDF = df.randomSplit([0.99,0.01])\n",
    "\n",
    "diff_in_train_test = trainDF.select('cls').subtract(testDF.select('cls')).distinct().show()\n",
    "\n",
    "#首先找到这些类，整理到一个列表\n",
    "not_exist_cls = trainDF.select('cls').subtract(testDF.select('cls')).distinct().rdd.map(lambda x :x[0]).collect()\n",
    "print(not_exist_cls)\n",
    "#定义一个方法，用于检测\n",
    "def should_remove(x):\n",
    "    if x in not_exist_cls:\n",
    "        return -1\n",
    "    else :\n",
    "        return x\n",
    "\n",
    "#创建udf，udf函数需要两个参数：\n",
    "# Function\n",
    "# Return type (in my case StringType())\n",
    "\n",
    "#在RDD中可以直接定义函数，交给rdd的transformatioins方法进行执行\n",
    "#在DataFrame中需要通过udf将自定义函数封装成udf函数再交给DataFrame进行调用执行\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "#这一步是封装\n",
    "check = udf(should_remove,StringType())\n",
    "\n",
    "resultDF = trainDF.withColumn('New_cls',check(trainDF['cls'])).filter('New_cls <> -1')\n",
    "\n",
    "resultDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON数据的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonString = [\n",
    "\"\"\"{ \"id\" : \"01001\", \"city\" : \"AGAWAM\",  \"pop\" : 15338, \"state\" : \"MA\" }\"\"\",\n",
    "\"\"\"{ \"id\" : \"01002\", \"city\" : \"CUSHMAN\", \"pop\" : 36963, \"state\" : \"MA\" }\"\"\"\n",
    "]\n",
    "jsonRDD = sc.parallelize(jsonString)   # stringJSONRDD\n",
    "jsonDF =  spark.read.json(jsonRDD)  # convert RDD into DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- pop: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark处理json会做类型推断\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+\n",
      "|   city|   id|  pop|state|\n",
      "+-------+-----+-----+-----+\n",
      "| AGAWAM|01001|15338|   MA|\n",
      "|CUSHMAN|01002|36963|   MA|\n",
      "+-------+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF = spark.read.json(\"/USA.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+--------------------+-------+\n",
      "|     _corrupt_record|            geometry|  id|          properties|   type|\n",
      "+--------------------+--------------------+----+--------------------+-------+\n",
      "|{\"type\":\"FeatureC...|                null|null|                null|   null|\n",
      "|                null|[WrappedArray(Wra...|  01|           [Alabama]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  02|            [Alaska]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  04|           [Arizona]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  05|          [Arkansas]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  06|        [California]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  08|          [Colorado]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  09|       [Connecticut]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  10|          [Delaware]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  11|[District of Colu...|Feature|\n",
      "|                null|[WrappedArray(Wra...|  12|           [Florida]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  13|           [Georgia]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  15|            [Hawaii]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  16|             [Idaho]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  17|          [Illinois]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  18|           [Indiana]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  19|              [Iowa]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  20|            [Kansas]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  21|          [Kentucky]|Feature|\n",
      "|                null|[WrappedArray(Wra...|  22|         [Louisiana]|Feature|\n",
      "+--------------------+--------------------+----+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# truncate=False\n",
    "jsonDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#去看每一部分的类型\n",
    "jsonDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+---+----------------+-------+\n",
      "|_corrupt_record|            geometry| id|      properties|   type|\n",
      "+---------------+--------------------+---+----------------+-------+\n",
      "|           null|[WrappedArray(Wra...| 41|        [Oregon]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 42|  [Pennsylvania]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 44|  [Rhode Island]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 45|[South Carolina]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 46|  [South Dakota]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 47|     [Tennessee]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 48|         [Texas]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 49|          [Utah]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 50|       [Vermont]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 51|      [Virginia]|Feature|\n",
      "+---------------+--------------------+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.filter(jsonDF.id>40).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF.createOrReplaceTempView(\"tmp_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+---+----------------+-------+\n",
      "|_corrupt_record|            geometry| id|      properties|   type|\n",
      "+---------------+--------------------+---+----------------+-------+\n",
      "|           null|[WrappedArray(Wra...| 41|        [Oregon]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 42|  [Pennsylvania]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 44|  [Rhode Island]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 45|[South Carolina]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 46|  [South Dakota]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 47|     [Tennessee]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 48|         [Texas]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 49|          [Utah]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 50|       [Vermont]|Feature|\n",
      "|           null|[WrappedArray(Wra...| 51|      [Virginia]|Feature|\n",
      "+---------------+--------------------+---+----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resultDF = spark.sql(\"select * from tmp_table where id>40\")\n",
    "resultDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- pop: long (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+-----+-------+-----+-----+\n",
      "|   id|   city|  pop|state|\n",
      "+-----+-------+-----+-----+\n",
      "|01001| AGAWAM|15338|   MA|\n",
      "|01002|CUSHMAN|36963|   MA|\n",
      "+-----+-------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonString = [\n",
    "\"\"\"{ \"id\" : \"01001\", \"city\" : \"AGAWAM\",  \"pop\" : 15338, \"state\" : \"MA\" }\"\"\",\n",
    "\"\"\"{ \"id\" : \"01002\", \"city\" : \"CUSHMAN\", \"pop\" : 36963, \"state\" : \"MA\" }\"\"\"\n",
    "]\n",
    "\n",
    "jsonRDD = sc.parallelize(jsonString)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#定义结构类型\n",
    "#StructType：schema的整体结构，表示JSON的对象结构\n",
    "#XXXStype:指的是某一列的数据类型\n",
    "jsonSchema = StructType() \\\n",
    "  .add(\"id\", StringType(),True) \\\n",
    "  .add(\"city\", StringType()) \\\n",
    "  .add(\"pop\" , LongType()) \\\n",
    "  .add(\"state\",StringType())\n",
    "\n",
    "# 如果类型不对，就会为null\n",
    "# jsonSchema = StructType() \\\n",
    "#   .add(\"id\", LongType(),True) \\\n",
    "#   .add(\"city\", StringType()) \\\n",
    "#   .add(\"pop\" , DoubleType()) \\\n",
    "#   .add(\"state\",StringType())\n",
    "\n",
    "reader = spark.read.schema(jsonSchema)\n",
    "\n",
    "jsonDF = reader.json(jsonRDD)\n",
    "jsonDF.printSchema()\n",
    "jsonDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  4| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1.删除重复数据\n",
    "\n",
    "groupby().count()：可以看到数据的重复情况\n",
    "'''\n",
    "df = spark.createDataFrame([\n",
    "  (1, 144.5, 5.9, 33, 'M'),\n",
    "  (2, 167.2, 5.4, 45, 'M'),\n",
    "  (3, 124.1, 5.2, 23, 'F'),\n",
    "  (4, 144.5, 5.9, 33, 'M'),\n",
    "  (5, 133.2, 5.7, 54, 'F'),\n",
    "  (3, 124.1, 5.2, 23, 'F'),\n",
    "  (5, 129.2, 5.3, 42, 'M'),\n",
    "], ['id', 'weight', 'height', 'age', 'gender'])\n",
    "\n",
    "# 查看重复记录\n",
    "#无意义重复数据去重：数据中行与行完全重复\n",
    "# 1.首先删除完全一样的记录,变为6条了\n",
    "df2 = df.dropDuplicates()\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+\n",
      "| id|weight|height|age|gender|\n",
      "+---+------+------+---+------+\n",
      "|  5| 133.2|   5.7| 54|     F|\n",
      "|  1| 144.5|   5.9| 33|     M|\n",
      "|  2| 167.2|   5.4| 45|     M|\n",
      "|  3| 124.1|   5.2| 23|     F|\n",
      "|  5| 129.2|   5.3| 42|     M|\n",
      "+---+------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#有意义去重：删除除去无意义字段之外的完全重复的行数据\n",
    "# 2.其次，关键字段值完全一模一样的记录（在这个例子中，是指除了id之外的列一模一样）\n",
    "# 删除某些字段值完全一样的重复记录，subset参数定义这些字段\n",
    "df3 = df2.dropDuplicates(subset = [c for c in df2.columns if c!='id'])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id_count=5, distinct_id_count=4)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3.有意义的重复记录去重之后，再看某个无意义字段的值是否有重复（在这个例子中，是看id是否重复）\n",
    "# 查看某一列是否有重复值，就是把id取出来，在distinct id取出来，对比\n",
    "import pyspark.sql.functions as fn\n",
    "df3.agg(fn.count('id').alias('id_count'),fn.countDistinct('id').alias('distinct_id_count')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+------+-------------+\n",
      "| id|weight|height|age|gender|       new_id|\n",
      "+---+------+------+---+------+-------------+\n",
      "|  5| 133.2|   5.7| 54|     F|  25769803776|\n",
      "|  1| 144.5|   5.9| 33|     M| 171798691840|\n",
      "|  2| 167.2|   5.4| 45|     M| 592705486848|\n",
      "|  3| 124.1|   5.2| 23|     F|1236950581248|\n",
      "|  5| 129.2|   5.3| 42|     M|1365799600128|\n",
      "+---+------+------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.对于id这种无意义的列重复，添加另外一列自增id，仅仅是增加一个无意义的自增id而已\n",
    "df3.withColumn('new_id',fn.monotonically_increasing_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "2.处理缺失值\n",
    "2.1 对缺失值进行删除操作(行，列)\n",
    "2.2 对缺失值进行填充操作(列的均值)\n",
    "2.3 对缺失值对应的行或列进行标记\n",
    "'''\n",
    "df_miss = spark.createDataFrame([\n",
    "(1, 143.5, 5.6, 28,'M', 100000),\n",
    "(2, 167.2, 5.4, 45,'M', None),\n",
    "(3, None , 5.2, None, None, None),\n",
    "(4, 144.5, 5.9, 33, 'M', None),\n",
    "(5, 133.2, 5.7, 54, 'F', None),\n",
    "(6, 124.1, 5.2, None, 'F', None),\n",
    "(7, 129.2, 5.3, 42, 'M', 76000),],\n",
    " ['id', 'weight', 'height', 'age', 'gender', 'income'])\n",
    "\n",
    "# 1.计算每条记录的缺失值情况\n",
    "\n",
    "df_miss.rdd.map(lambda row:(row['id'],sum([c==None for c in row]))).collect()\n",
    "# [(1, 0), (2, 1), (3, 4), (4, 1), (5, 1), (6, 2), (7, 0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|id_missing|    weight_missing|height_missing|       age_missing|    gender_missing|    income_missing|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "|       0.0|0.1428571428571429|           0.0|0.2857142857142857|0.1428571428571429|0.7142857142857143|\n",
      "+----------+------------------+--------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.计算各列的缺失情况百分比  fn.count(c)这一列多少个有值，fn.count(c)这个c为None不会记上的\n",
    "df_miss.agg(*[(1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing') for c in df_miss.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, weight: double, height: double, age: bigint, gender: string]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3、删除缺失值过于严重的列\n",
    "# 其实是先建一个DF，不要缺失值的列\n",
    "df_miss_no_income = df_miss.select([c for c in df_miss.columns if c != 'income'])\n",
    "\n",
    "df_miss_no_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+------+\n",
      "| id|weight|height| age|gender|\n",
      "+---+------+------+----+------+\n",
      "|  1| 143.5|   5.6|  28|     M|\n",
      "|  2| 167.2|   5.4|  45|     M|\n",
      "|  4| 144.5|   5.9|  33|     M|\n",
      "|  5| 133.2|   5.7|  54|     F|\n",
      "|  6| 124.1|   5.2|null|     F|\n",
      "|  7| 129.2|   5.3|  42|     M|\n",
      "+---+------+------+----+------+\n",
      "\n",
      "+---+------------------+-----------------+----+\n",
      "| id|            weight|           height| age|\n",
      "+---+------------------+-----------------+----+\n",
      "|4.0|140.28333333333333|5.471428571428571|40.4|\n",
      "+---+------------------+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4、按照缺失值删除行（threshold是根据一行记录中，缺失字段的百分比的定义,一行有3个缺少，代表要删除）\n",
    "df_miss_no_income.dropna(thresh=3).show()\n",
    "\n",
    "# 5、填充缺失值，可以用fillna来填充缺失值，\n",
    "# 对于bool类型、或者分类类型，可以为缺失值单独设置一个类型，missing\n",
    "# 对于数值类型，可以用均值或者中位数等填充\n",
    "\n",
    "# fillna可以接收两种类型的参数：\n",
    "# 一个数字、字符串，这时整个DataSet中所有的缺失值都会被填充为相同的值。\n",
    "# 也可以接收一个字典｛列名：值｝这样\n",
    "\n",
    "# 先计算均值\n",
    "means = df_miss_no_income.agg( *[fn.mean(c).alias(c) for c in df_miss_no_income.columns if c != 'gender']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 4.0,\n",
       " 'weight': 140.28333333333333,\n",
       " 'height': 5.471428571428571,\n",
       " 'age': 40.4}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "# 先计算均值，并组织成一个字典\n",
    "means = df_miss_no_income.agg( *[fn.mean(c).alias(c) for c in df_miss_no_income.columns if c != 'gender']).toPandas().to_dict('records')[0]\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------+---+-------+\n",
      "| id|            weight|height|age| gender|\n",
      "+---+------------------+------+---+-------+\n",
      "|  1|             143.5|   5.6| 28|      M|\n",
      "|  2|             167.2|   5.4| 45|      M|\n",
      "|  3|140.28333333333333|   5.2| 40|missing|\n",
      "|  4|             144.5|   5.9| 33|      M|\n",
      "|  5|             133.2|   5.7| 54|      F|\n",
      "|  6|             124.1|   5.2| 40|      F|\n",
      "|  7|             129.2|   5.3| 42|      M|\n",
      "+---+------------------+------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 然后添加其它的列\n",
    "means['gender'] = 'missing'\n",
    "\n",
    "df_miss_no_income.fillna(means).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight': [91.69999999999999, 191.7], 'height': [4.499999999999999, 6.1000000000000005], 'age': [-11.0, 93.0]}\n",
      "+---+--------+--------+-----+\n",
      "| id|weight_o|height_o|age_o|\n",
      "+---+--------+--------+-----+\n",
      "|  1|   false|   false|false|\n",
      "|  2|   false|   false|false|\n",
      "|  3|    true|   false| true|\n",
      "|  4|   false|   false|false|\n",
      "|  5|   false|   false|false|\n",
      "|  6|   false|   false|false|\n",
      "|  7|   false|   false|false|\n",
      "+---+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3、异常值处理\n",
    "异常值：不属于正常的值 包含：缺失值，超过正常范围内的较大值或较小值\n",
    "分位数去极值\n",
    "中位数绝对偏差去极值\n",
    "正态分布去极值\n",
    "上述三种操作的核心都是：通过原始数据设定一个正常的范围，超过此范围的就是一个异常值\n",
    "'''\n",
    "df_outliers = spark.createDataFrame([\n",
    "(1, 143.5, 5.3, 28),\n",
    "(2, 154.2, 5.5, 45),\n",
    "(3, 342.3, 5.1, 99),\n",
    "(4, 144.5, 5.5, 33),\n",
    "(5, 133.2, 5.4, 54),\n",
    "(6, 124.1, 5.1, 21),\n",
    "(7, 129.2, 5.3, 42),\n",
    "], ['id', 'weight', 'height', 'age'])\n",
    "# 设定范围 超出这个范围的 用边界值替换\n",
    "\n",
    "# approxQuantile方法接收三个参数：参数1，列名；参数2：想要计算的分位点，可以是一个点，也可以是一个列表（0和1之间的小数），第三个参数是能容忍的误差，如果是0，代表百分百精确计算。\n",
    "\n",
    "cols = ['weight', 'height', 'age']\n",
    "\n",
    "bounds = {}\n",
    "for col in cols:\n",
    "    quantiles = df_outliers.approxQuantile(col, [0.25, 0.75], 0.05)\n",
    "    IQR = quantiles[1] - quantiles[0]\n",
    "    bounds[col] = [\n",
    "        quantiles[0] - 1.5 * IQR,\n",
    "        quantiles[1] + 1.5 * IQR\n",
    "        ]\n",
    "\n",
    "print(bounds)\n",
    "# {'age': [-11.0, 93.0], 'height': [4.499999999999999, 6.1000000000000005], 'weight': [91.69999999999999, 191.7]}\n",
    "\n",
    "# 为异常值字段打标志,如果小于下线，或者大于上线\n",
    "outliers = df_outliers.select(*['id'] + [( (df_outliers[c] < bounds[c][0]) | (df_outliers[c] > bounds[c][1]) ).alias(c + '_o') for c in cols ])\n",
    "outliers.show()\n",
    "#\n",
    "# +---+--------+--------+-----+\n",
    "# | id|weight_o|height_o|age_o|\n",
    "# +---+--------+--------+-----+\n",
    "# |  1|   false|   false|false|\n",
    "# |  2|   false|   false|false|\n",
    "# |  3|    true|   false| true|\n",
    "# |  4|   false|   false|false|\n",
    "# |  5|   false|   false|false|\n",
    "# |  6|   false|   false|false|\n",
    "# |  7|   false|   false|false|\n",
    "# +---+--------+--------+-----+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|weight|\n",
      "+---+------+\n",
      "|  3| 342.3|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 再回头看看这些异常值的值，重新和原始数据关联\n",
    "\n",
    "df_outliers = df_outliers.join(outliers, on='id')\n",
    "df_outliers.filter('weight_o').select('id', 'weight').show()\n",
    "# +---+------+\n",
    "# | id|weight|\n",
    "# +---+------+\n",
    "# |  3| 342.3|\n",
    "# +---+------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|age|\n",
      "+---+---+\n",
      "|  3| 99|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#这个是年龄的\n",
    "df_outliers.filter('age_o').select('id', 'age').show()\n",
    "# +---+---+\n",
    "# | id|age|\n",
    "# +---+---+\n",
    "# |  3| 99|\n",
    "# +---+---+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}